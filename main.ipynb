{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27378,"status":"ok","timestamp":1605109567523,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"JBMONQERPHXK","outputId":"e4d7350b-c840-4e91-913e-b115f6fe4888"},"outputs":[{"name":"stdout","output_type":"stream","text":["unzip:  cannot find or open resume_corpus/resume_samples.zip, resume_corpus/resume_samples.zip.zip or resume_corpus/resume_samples.zip.ZIP.\n","unzip:  cannot find or open resume_corpus/resumes_corpus.zip, resume_corpus/resumes_corpus.zip.zip or resume_corpus/resumes_corpus.zip.ZIP.\n"]}],"source":["! unzip resume_corpus/resume_samples.zip -d resume_corpus\n","! unzip resume_corpus/resumes_corpus.zip -d resume"]},{"cell_type":"markdown","metadata":{"id":"SAKxB93K4NAk"},"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"EnVcsBiIPzqZ"},"outputs":[],"source":["# make word2vec model which will be used for converting resume to numerical data"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1172,"status":"ok","timestamp":1605109462287,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"OTf-bGgLQkhZ","outputId":"470b33f2-a567-4cd8-9be7-12714b9a1ada"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Error loading punkt: <urlopen error [SSL:\n","[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n","[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n","[nltk_data] Error loading stopwords: <urlopen error [SSL:\n","[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n","[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"]}],"source":["from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","import string\n","from nltk import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from tqdm import tqdm\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.tokenize import RegexpTokenizer\n","import hashlib\n","import spacy\n","from collections import Counter\n"," \n","MY_DIR = '/content/drive/My Drive/Resume Classification'\n","MY_DIR2 = '/content/drive/My Drive/Resume Classification2'\n","# nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n","skills_path = MY_DIR + '/data/skills.txt'"]},{"cell_type":"markdown","metadata":{"id":"ZU2fNmRv4PWf"},"source":["created to covnert resume text to tokens after removing stopwords and punctuations from text."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"TmJ8F7OqTE-a"},"outputs":[],"source":["from keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n","from keras.models import Sequential\n","import keras\n","from keras.optimizers import SGD\n","from sklearn.model_selection import train_test_split\n","from keras import layers"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1330,"status":"ok","timestamp":1605077301810,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"xkaQBoUmXpIA","outputId":"8a7d93f5-5f64-494c-f108-7f85399d4c91"},"outputs":[{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/alishahrokhi/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n","File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/alishahrokhi/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[39mreturn\u001b[39;00m filtered_sentence\n\u001b[1;32m     16\u001b[0m resumetext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHarsh is a great boy , and he loves ; to play basketball with Manish. #include<iostream> An\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mprint\u001b[39m(removestop_words(resumetext))\n\u001b[1;32m     20\u001b[0m \u001b[39m# print(jointoken(removestop_words(resumetext)))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# print(get_tokens(resumetext));\u001b[39;00m\n","Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mremovestop_words\u001b[0;34m(resumetext)\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m RegexpTokenizer(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m texttokens \u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mtokenize(resumetext)\n\u001b[0;32m----> 6\u001b[0m stop_words \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m tokens_without_sw \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m texttokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stop_words]\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m tokens_without_sw\n","File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n","File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n","File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/alishahrokhi/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}],"source":["#word tokenize and remove stopwords\n"," \n","def removestop_words(resumetext):\n","  tokenizer = RegexpTokenizer(r'\\w\\w+')\n","  texttokens =tokenizer.tokenize(resumetext)\n","  stop_words = stopwords.words(\"english\")\n","  tokens_without_sw = [word for word in texttokens if not word in stop_words]\n","  \n","  return tokens_without_sw\n"," \n","def jointoken(word_tokens):\n","  filtered_sentence = (\" \").join(word_tokens)\n","  return filtered_sentence\n"," \n"," \n","resumetext = \"Harsh is a great boy , and he loves ; to play basketball with Manish. #include<iostream> An\"\n"," \n","print(removestop_words(resumetext))\n"," \n","# print(jointoken(removestop_words(resumetext)))\n","# print(get_tokens(resumetext));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwWLRQRoAdVT"},"outputs":[],"source":["def domain_word_selection(resumetext):\n","  tokenizer = RegexpTokenizer(r'\\w\\w+')\n","  texttokens =tokenizer.tokenize(resumetext)\n","  \n","  stop_words = stopwords.words(\"english\")\n","  tokens_without_sw = [word for word in texttokens if not word in stop_words]\n","\n","  for token in tokens_without_sw:\n","    try:\n","      if itemData[token]:\n","        pass\n","    except:\n","        tokens_without_sw.remove(token)  \n","  return tokens_without_sw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SigRzlxYfNrf"},"outputs":[],"source":["# import codecs\n","# tokenizer = RegexpTokenizer('\\/|^\\.|\\.$|,|;|\\(|\\)|^\\-|\\-$|:|;', gaps=True)\n","\n","# skills_file = codecs.open(skills_path, \"rU\", encoding='utf-8', errors='ignore')\n","# tokens_in = set(tokenizer.tokenize(skills_file.read()))\n","# tokens_in = [token.lower() for token in tokens_in]\n","# # tokens_in = dict(zip(tokens_in, len(tokens_in)))\n","# dict = {}\n","\n","# for token in tokens_in:\n","#   if token.lower() == 'dell\\t':\n","#     print('yes')\n","#   dict[token] = len(token)\n","#   # print(token)\n","# print(tokens_in[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxQ6BLGngjcC"},"outputs":[],"source":["for key, value in tokens_in.items():\n","  print(key, value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pi6Oil2I64q3"},"outputs":[],"source":["def domain_specific():\n","  skills_file = codecs.open(skills_path, \"rU\", encoding='utf-8', errors='ignore')\n","  tokens_in = set(tokenizer.tokenize(skills_file.read()))\n","  tokens_in = [token.lower() for token in tokens_in]\n","  tokens_in = dict(zip(tokens_in, range(len(tokens_in))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pwt1HtuH70jm"},"outputs":[],"source":[" def init_outputs():\n","      y_dict = dict()\n","      data_y_file.seek(0)\n","      lines = data_y_file.read().splitlines()\n","      tokens_out = []\n","      for line in lines :\n","          line_items = line.split(\":\")\n","          y_dict.update({line_items[0].lower():line_items[1].split(\",\")})\n","          tokens_out+=line_items[1].split(\",\")\n","      return y_dict, Counter(tokens_out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQ0K_FNW79EJ"},"outputs":[],"source":["def filter_tokens_out(freq):\n","        filtered_tokens = [t for t,v in tokens_out.items() if v >= freq]\n","        return filtered_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJEbNjJWLYfC"},"outputs":[],"source":["   def get_tokens(text):\n","        sents = nltk.sent_tokenize(text)\n","        tokens = [token.lemma_ for sent in sents for token in nlp(sent.lower())\n","                  if token.lemma_ not in stopset and len(token.lemma_) > 2\n","                  and (token.lemma_ is not \":::\" or token.lemma_ is not \"::::::\")]\n","        tokens = Counter(tokens)\n","        return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PHM1UhzA6VK"},"outputs":[],"source":["import json\n","with open('/content/drive/My Drive/Resume Classification/data/tokens_in.json') as json_file:\n","    itemData = json.load(json_file)"]},{"cell_type":"markdown","metadata":{"id":"qoDfsO1E4lpv"},"source":["## Storing resume text to corpus before applying Word 2 Vec i.e Skip gram Algo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":137764,"status":"ok","timestamp":1605109819377,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"5yi9MutTQsno","outputId":"a6bf8a7d-0a7c-4a4c-b7a2-bf3cdd5bc65b"},"outputs":[],"source":["# read all resumes to make learn word2vec\n","corpus = []\n","with open('resume_corpus/resume_samples.txt', encoding=\"ISO-8859-1\") as file:\n","  for line in tqdm(file):\n","    text = line.split(':::')[-1].rstrip()\n","    text = domain_word_selection(text)\n","    corpus.append(text)\n"," \n","corpus[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jz-dQkSO7-Ov"},"outputs":[],"source":["corpus = np.load('{}/corpus.npy'.format(MY_DIR), allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EOSxCCu63h6"},"outputs":[],"source":["np.save('{}/corpus.npy'.format(MY_DIR), corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taPsvuz_nd2z"},"outputs":[],"source":["with open('input.json') as json_file:\n","    itemData = json.load(json_file)"]},{"cell_type":"markdown","metadata":{"id":"ohDhc6dK5A2-"},"source":["**We have taken mean of the dimensions of the vector and that mean will be fixed dimension of our vec**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1234,"status":"ok","timestamp":1605110712428,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"iIa5VmrVSi4k","outputId":"89d54528-2b25-41ce-fc22-e0d01c7c5b25"},"outputs":[],"source":["print(len(corpus))\n","length = np.array([len(i) for i in corpus])\n","mean = length.mean()\n","\n","print('Average count :',mean)\n","print('Max word count :', length.max(),'\\nMin word count :', length.min())\n","cnt = 0\n","for i in length:\n","  if i > 1000:\n","    cnt+=1\n","\n","print(cnt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Anyk2IkYCrq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28192,"status":"ok","timestamp":1605109259467,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"CB5Qh7gkpN9u","outputId":"0611d695-b9cf-4050-f2d0-9d937e165ae0"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"vDASRF-P5byR"},"source":["**Creating Word to vec Skip Gram Algo model of size 100 and window 5 as given  in paper**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXGYGOIgZDE5"},"outputs":[],"source":["# model with embedding size 100\n","model_sg = Word2Vec(corpus, min_count=1, size=100, sg=1, window=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v63Y8Dif1fJ8"},"outputs":[],"source":["# model with embedding size 32\n","model_sg = Word2Vec(corpus, size=32, min_count=1, sg=1, window=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":920,"status":"ok","timestamp":1605078295240,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"KEWFH87JZSmU","outputId":"7ef1988d-68b5-49f7-c60f-5b73dcfaff73"},"outputs":[],"source":["word = 'vue'\n","# print(model.wv.most_similar(word))\n","print(model_sg.wv.most_similar(word))"]},{"cell_type":"markdown","metadata":{"id":"BBO4op5c5ley"},"source":["**We have saved the model to below location so no  need to run the model again simply run below to use our Word to vec embeddings**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa4XjNVRc51U"},"outputs":[],"source":["# save model\n","# model.save('/content/drive/My Drive/Resume Classification/model_wv.32.model')\n","model_sg.save('/content/drive/My Drive/Resume Classification/model_sg_wv.100d.model')\n","# model_sg.save('/content/drive/My Drive/Resume Classification2/model_sg_wv.100d.model')"]},{"cell_type":"markdown","metadata":{"id":"sW4X9atz-7u-"},"source":["**We have taken 32  and not 100 as it was not running for such large dataset of size 29784 so at the end we will use online gpu to create 100....**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iunGxZPNqcql"},"outputs":[],"source":["model_sg = Word2Vec().wv.load('/content/drive/My Drive/Resume Classification/model_sg_wv.100d.model')\n","wv = model_sg.wv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4331,"status":"ok","timestamp":1604853605479,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"aJxi1_V5HGmI","outputId":"2a8c63df-c2cd-4f11-8973-ef7acd39b33a"},"outputs":[],"source":["#len(corpus)\n","wv['javascript']"]},{"cell_type":"markdown","metadata":{"id":"0ZX2CSmd55fU"},"source":["**As Given in Pdf we have created an empty matrix and vector whose dimension is not L we simply add padding and those whose dimension is greater we ignore**"]},{"cell_type":"markdown","metadata":{"id":"nojeiaEx6OkN"},"source":["**Here we have processed label for database administrator and stored our npy file for future use**\n","\n","\n","1.  We have to create for 9 more y labels i.e for 9 more classes\n","We wil run this for 9 more class everytime channging label of the class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCDq92CBe4Pi"},"outputs":[],"source":["low__,high__ = 1,29784\n","pre=0.84\n"," \n","import random\n","from sklearn.utils import shuffle\n"," \n","def process_resume_data(dirpath, L, size=32):\n"," \n","  X = []\n"," \n","  for i in tqdm(range(low__,high__)):\n","    with open('{}/{:05d}.txt'.format(dirpath, i), encoding='ISO-8859-1') as f:\n","      line = f.readline()\n","      line = removestop_words(line)\n","      x = []\n","      length = L\n","      for word in line:\n","        if length > 0:\n","          # if word in model.wv:\n","          try:\n","            x.append(np.array(wv[word], dtype=np.float32))\n","            length -= 1\n","          except:\n","            pass\n","        else:\n","          break\n"," \n","      while length > 0:\n","        x.append(np.zeros(size, dtype=np.float32))\n","        length -= 1\n","      x = np.array(x)\n","      X.append(x)\n"," \n","  X = np.array(X)\n","  return X\n"," \n","\n","def process_labels(dirpath, label):\n","  y = []\n","  LABEL = label\n","  for i in tqdm(range(low__, high__)):\n","    flag = False\n","    with open('{}/{:05d}.lab'.format(dirpath, i)) as f:\n","      for line in f:\n","        if line == LABEL:\n","          flag = True\n"," \n","    \n","    if flag == True:\n","      y.append(1)\n","    else:\n","      y.append(0)\n","  return np.array(y, dtype='int8')\n","\n"," \n"," \n","def process_training_data(path, label, L, size=100, rc=29783, from_cache=False):\n","  \"\"\"\n","    return (x,y) for corresponding label\n","  \"\"\"\n","  if from_cache:\n","    return load_train_data(MY_DIR, label, L)\n"," \n","  rc += 1\n","  ids = [i for i in range(1,rc)]\n","  X = []\n","  y = []\n"," \n","  # read .lab file if we find to be equal to label, add corresponding x and remove from ids\n","  print('Reading positive samples...')\n","  __count = 0\n","  for i in tqdm(range(1, rc)):\n","    if __count > 13000:\n","      # break\n","      # instead of break lets remove all positive example\n","      flag = False\n","      with open('{}/{:05d}.lab'.format(path, i)) as f:\n","        for line in f:\n","          line = line.rstrip()\n","          if line == label:\n","            flag = True\n","\n","      if flag:\n","        ids.remove(i)\n","        continue\n"," \n","    flag = False\n","    with open('{}/{:05d}.lab'.format(path, i)) as f:\n","      for line in f:\n","        line = line.rstrip()\n","        if line == label:\n","          flag = True\n"," \n","    if flag:\n","      y.append(1)\n","      __count += 1\n"," \n","      ids.remove(i)\n"," \n","      # read corresponding corpus\n","      with open('{}/{:05d}.txt'.format(path, i), encoding='ISO-8859-1') as f:\n","        line = f.readline()\n","        # line = removestop_words(line)\n","        line = domain_word_selection(line)\n","        x = []\n","        length = L\n","        for word in line:\n","          if length > 0:\n","            if word in wv:\n","              x.append(np.array(wv[word], dtype=np.float32))\n","              length -= 1\n","          else:\n","            break\n","  \n","        while length > 0:\n","          x.append(np.zeros(size, dtype=np.float32))\n","          length -= 1\n","        x = np.array(x)\n","        X.append(x)\n","  \n"," \n","  # choose equal random negative samples \n","  n = len(X)\n","  negatives = random.choices(ids, k=n)\n"," \n","  print('Reading negative samples...')\n","  for i in tqdm(negatives):\n","    # append y\n","    y.append(0)\n"," \n","    # append x\n","    with open('{}/{:05d}.txt'.format(path, i), encoding='ISO-8859-1') as f:\n","        line = f.readline()\n","        # line = removestop_words(line)\n","        line = domain_word_selection(line)\n","        x = []\n","        length = L\n","        for word in line:\n","          if length > 0:\n","            if word in wv:\n","              x.append(np.array(wv[word], dtype=np.float32))\n","              length -= 1\n","          else:\n","            break\n","  \n","        while length > 0:\n","          x.append(np.zeros(size, dtype=np.float32))\n","          length -= 1\n","        x = np.array(x)\n","        X.append(x)\n"," \n"," \n","  # shuffle all X and y\n","  X = np.array(X)\n","  y = np.array(y)\n","  print('Shuffling...')\n","  X, y = shuffle(X, y)\n","  print('All done...')\n","  return X, y\n","  \n"," \n","d = 100\n","L = 500\n","# x = process_resume_data('resume', L=L, size=d)\n","# np.save('{}/training/training_x_{}X{}.npy'.format(MY_DIR, L, d), x)\n","# y = process_labels('resume', 'Front_End_Developer')\n","# x, y = process_training_data('resume', 'Web_Developer', L, size=d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwYaMK2tNdQk"},"outputs":[],"source":["import json\n"," \n","def load_train_data(path, label, L=1000, d=100):\n","  filepath_x = \"{}/training/train_x_{}_{}X{}.npy\".format(path, label, L, d)\n","  filepath_y = \"{}/training/train_y_{}.npy\".format(path, label)\n"," \n","  x = np.load(filepath_x, allow_pickle=True)\n","  y = np.load(filepath_y, allow_pickle=True)\n"," \n","  return x, y\n"," \n","def save_train_data(path, label, x, y):\n","  n, L, d = x.shape\n","  filepath_x = \"{}/training/train_x_{}_{}X{}.npy\".format(path, label, L, d)\n","  filepath_y = \"{}/training/train_y_{}.npy\".format(path, label)\n"," \n","  np.save(filepath_x, x)\n","  np.save(filepath_y, y)\n","  print('Data saved...')\n"," \n","def load_model(path, label):\n","  # for neural network\n","  model_path = \"{}/models/{}.h5\".format(path, label)\n","  model = keras.models.load_model(model_path)\n"," \n","  history_path = \"{}/models/{}.history.json\".format(path, label)\n","  # with open(history_path, 'r') as f:\n","  #   model.history['history'] = json.loads(f.read())\n","  return model\n","\n","class History:\n","  def __init__(self, history):\n","    self.history = history\n","\n","def load_history(path, label):\n","  history_path = \"{}/models/{}.history.json\".format(path, label)\n","  \n","  with open(history_path, 'r') as f:\n","    history = json.loads(f.read())\n","    hist = History(history)\n","\n","  return hist\n"," \n","def save_model(path, label, model):\n","  # for neural network\n","  model_path = \"{}/models/{}.h5\".format(path, label)\n"," \n","  model.save(model_path)\n"," \n","  # save history\n","  history_path = \"{}/models/{}.history.json\".format(path, label)\n","  with open(history_path, 'w') as f:\n","    json.dump(model.history.history, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QciOA98K7riV"},"outputs":[],"source":["np.save('{}/training/training_x_{}X{}.npy'.format(MY_DIR, L, d), x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKuOC2_0fUY9"},"outputs":[],"source":["x = np.load('{}/training_x.npy'.format(MY_DIR), allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1029,"status":"ok","timestamp":1604493914679,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"6I_l9d0ae-a_","outputId":"b09e6e10-cf84-41cd-a21d-a17c3eef787c"},"outputs":[],"source":["print(x.shape)\n","print(x.dtype)\n","print(y.shape)\n","print(y.dtype)\n","print(np.count_nonzero(y))\n","# print(len(x))\n","# x = np.array(x)\n","# y = np.array(y)\n","# np.save('{}/training_x.npy'.format(MY_DIR), x)\n","n, L, d = x.shape\n","print(L,d)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZzJzvpyAPY1"},"outputs":[],"source":["# make neural network with convolution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggG9CANauV_Y"},"outputs":[],"source":["x,y = load_train_data(MY_DIR, classes[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3x5iqHCWLNZ"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=73)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DV15_exWhGH"},"outputs":[],"source":["model = Sequential()\n","# model1.add(layers.Conv2D(100,kernel_size=(1,d), input_shape=(L,d), activation='relu'))\n","model.add(layers.Conv1D(100,1, input_shape=(L,d), activation='relu'))\n","model.add(layers.GlobalMaxPool1D())\n","#model.add(layers.Flatten())\n","model.add(layers.Dense(10, activation='relu'))\n","# model1.add(layers.Dense(32, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","sgd = SGD(lr=0.01, decay=1e-6, momentum=0.1, nesterov=True)\n","model.compile(loss='binary_crossentropy',optimizer=sgd,metrics=['accuracy'])\n"," \n","# model.compile()\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOI-J5FteccY"},"outputs":[],"source":["model = load_model('{}/mymodal_{}.h5'.format(MY_DIR, 0.95))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yscWjvdcSPBE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UF2x5AIWjQ7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjXTEngHYddW"},"outputs":[],"source":["history  = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQPNK_lwu1Qm"},"outputs":[],"source":["plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gI2uHh2dYj-3"},"outputs":[],"source":["model.save('{}/mymodal_{}.h5'.format(MY_DIR, 0.97))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4586,"status":"ok","timestamp":1604493063080,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"Vtqz-85VUP-1","outputId":"bf341131-085d-4178-9fbf-c12bb8310378"},"outputs":[],"source":["y_pred = model.predict(x)\n","y_pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1036,"status":"ok","timestamp":1604493086777,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"L1cDhTBqNqz1","outputId":"80cc507f-95ea-4ba5-87c1-3ed2a5376b46"},"outputs":[],"source":["y_pred = y_pred > 0.5\n","cnt = 0\n","for i in y_pred:\n","  if i==True:\n","    cnt += 1\n","\n","print(cnt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQIIar3WNvCB"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","cm = confusion_matrix(y, y_pred)\n","cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4XuiMjM5nge"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","f1_score(y, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"-TRhgEZi0Zil"},"source":["# Data Normalization \n","### Using HashLib MD5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdaYFJSJ0eGX"},"outputs":[],"source":["\n","with open('/content/drive/My Drive/Resume Classification/data/normlized_classes.txt', encoding=\"ISO-8859-1\") as file:\n","  dict = {'harsh':'raj'} #for testing added this data\n","  for line in tqdm(file):\n","    text = line.split(':')\n","    # print(hashlib.md5(text[0].encode()).hexdigest())\n","    for a in text1:\n","      dict.__setitem__(hashlib.md5(a.encode()).hexdigest(),text[1][:-1]) \n","    \n","\n","    \n","print(dict)\n","x='python full stack developer'\n","dict[hashlib.md5(x.encode()).hexdigest()]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2496,"status":"ok","timestamp":1604521961804,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"Sb-IdFRH0uag","outputId":"52a5d4f3-d740-4af0-9557-cddcf4451c13"},"outputs":[],"source":["with open('/content/drive/My Drive/Resume Classification/data/resume_samples.txt', encoding=\"ISO-8859-1\") as file:\n","  label=[]\n","  i=0\n","  for line in tqdm(file):\n","    i=i+1\n","    text = line.split(':::')[1]\n","    resume=line.split(':::')[2]\n","    label.append(resume)\n","    labels=text.split(';')\n","    writelabels=['unicorn']\n","    for xlabels in labels:\n","      hash=hashlib.md5(xlabels.encode()).hexdigest();\n","      if hash in dict:\n","        if dict[hash] in writelabels:\n","          continue\n","        else:\n","          print(dict[hash]+'----',i)\n","          writelabels.append(dict[hash])             "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjyldtz_7jUx"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KS4pv8PzeI_A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qE6cdVHqTcgt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bjhta9GNUZ-F"},"source":["# Training and Prediction for all Different 10 Labels for purpose of this project.\n","\n","we have 10 labels to train "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcwlprKnUB08"},"outputs":[],"source":["import matplotlib.pyplot as plt\n"," \n","def plot_history(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5alOM1JUpKz"},"outputs":[],"source":["classes = [\"Software_Developer\", \"Front_End_Developer\", \"Network_Administrator\", \"Web_Developer\", \"Project_manager\", \"Database_Administrator\", \"Security_Analyst\",\n","           \"Systems_Administrator\", \"Python_Developer\", \"Java_Developer\"]\n","L, d = 500,100"]},{"cell_type":"markdown","metadata":{"id":"4qM04yXXNFG4"},"source":["**Creating sub dataset for each binary classification CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Af95r_8vULoI"},"outputs":[],"source":["for label in classes[:1]:\n","  print(\"For label : \", label)\n","  # get training data\n","  x, y = process_training_data('resume', label, L, size=d)\n","  print(x.shape)\n","  save_train_data(MY_DIR, label, x, y)\n","  del x\n","  del y"]},{"cell_type":"markdown","metadata":{"id":"KKOvelYeNZ-3"},"source":["**Training each CNN classifier on its sub-dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1304950,"status":"ok","timestamp":1604991225314,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"6cgbbqt6U6mM","outputId":"1eade77e-6e3f-4814-9cb9-5fc29ce5737a"},"outputs":[],"source":["for label in classes[:1]:\n","  print(\"Loading train data\", label)\n","  x,y = process_training_data('resume', label, L, from_cache=True)\n","  print('Data loaded...')\n"," \n","  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=1000)\n"," \n","  model = Sequential()\n","  model.add(layers.Conv1D(100,1, input_shape=(L,d), activation='relu'))\n","  model.add(layers.GlobalMaxPool1D())\n","  model.add(layers.Dense(10, activation='relu'))\n","  model.add(layers.Dense(1, activation='sigmoid'))\n","  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.1, nesterov=True)\n","  model.compile(loss='binary_crossentropy',optimizer=sgd,metrics=['accuracy'])\n","  \n","  print(model.summary())\n"," \n","  history = model.fit(x_train, y_train, epochs=60, batch_size=64, validation_data=(x_test, y_test))\n"," \n","  plot_history(history)\n"," \n","  save_model(MY_DIR, label, model)\n"," \n","  del x_train\n","  del x_test\n","  del y_train\n","  del y_test\n","  del x\n","  del y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Oj0njaBjCIH"},"outputs":[],"source":["y_pred = model.predict(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPUf_DoLwuKc"},"outputs":[],"source":["del x_train\n","del x_test\n","del y_train\n","del y_test\n","del x\n","del y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O8RYm2NwwM9"},"outputs":[],"source":["def debug_metrics():\n","  loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n","  print(\"************************Results for class :\"+str(curr_class)+\"*********************\")\n","  print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","  loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n","  print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n","  loss, accuracy = model.evaluate(X_test_ones, Y_test_ones, verbose=False)\n","  print(\"Testing Accuracy of class-1:  {:.4f}\".format(accuracy))\n","  loss, accuracy = model.evaluate(X_test_zeros, Y_test_zeros, verbose=False)\n","  print(\"Testing Accuracy of class-0:  {:.4f}\".format(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6775,"status":"ok","timestamp":1604993746751,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"AivwElDtXxzh","outputId":"c2898076-6b21-4450-ad55-33a9b0ae3c33"},"outputs":[],"source":["for label in classes:\n","  hist = load_history(MY_DIR, label)\n","  plot_history(hist)\n","  print(label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQopIuJHX1Ri"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExSA7jWk57WC"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"o80NCuAu58iW"},"source":["# Testing and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBiS4oQECQ5J"},"outputs":[],"source":["labels = {\"Software_Developer\": 0, \"Front_End_Developer\": 1, \"Network_Administrator\": 2,\n","           \"Web_Developer\": 3, \"Project_manager\": 4, \"Database_Administrator\": 5, \"Security_Analyst\": 6,\n","           \"Systems_Administrator\": 7, \"Python_Developer\": 8, \"Java_Developer\": 9}"]},{"cell_type":"markdown","metadata":{"id":"DpVZADAlNryA"},"source":["**Dividing data for testing in 3 parts as all data can't be loaded in RAM at a time**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYM_cfDG5_mK"},"outputs":[],"source":["# process x and y in parts for evaluation \n","def process_testing_data(path, dest, count=7500, L=1000, size=100, rc=29783):\n","  \"\"\"\n","    path : directory path where to read file\n","    dest : directory of destination path to save file\n","    count : number of data each file\n","  \"\"\"\n","  i,j = 1,1+count\n","  part = 1\n","  while i<rc:\n","    j = min(j, rc+1)\n","\n","    X = []\n","    y = []\n","    print('Processing part :', part)\n","    for k in tqdm(range(i,j)):\n","      # read x\n","      with open('{}/{:05d}.txt'.format(path, k), encoding='ISO-8859-1') as f:\n","        line = f.readline()\n","        line = domain_word_selection(line)\n","        x = []\n","        length = L\n","        for word in line:\n","          if length > 0:\n","            if word in wv:\n","              x.append(np.array(wv[word], dtype=np.float32))\n","              length -= 1\n","          else:\n","            break\n","  \n","        while length > 0:\n","          x.append(np.zeros(size, dtype=np.float32))\n","          length -= 1\n","        x = np.array(x)\n","        X.append(x)\n","      \n","      temp = [0 for i in range(10)]\n","      with open('{}/{:05d}.lab'.format(path, k)) as f:\n","        for line in f:\n","          index = labels[line.rstrip()] # this is where newline makes difference\n","          temp[index] = 1\n","\n","      temp = np.array(temp)\n","      y.append(temp)\n","\n","    # write X and y\n","    print(\"Saving part\", part)\n","    save_testing_data(dest, part, X, y)\n","    \n","    # free ram by deleting\n","    del X\n","    del y\n","\n","\n","\n","    i = j\n","    j = i+ count\n","    part += 1\n","\n","process_testing_data('resume', MY_DIR, count=10000, L=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7xk8QolEZWT"},"outputs":[],"source":["# load testing data\n","def load_testing_data(path, part_no=1):\n","  x_path = \"{}/testing/data_x_{}.npy\".format(path, part_no)\n","  y_path = '{}/testing/data_y_{}.npy'.format(path, part_no)\n","\n","  x = np.load(x_path, allow_pickle=True)\n","  y = np.load(y_path, allow_pickle=True)\n","\n","  return x,y\n","  \n","# save testing data\n","def save_testing_data(path, part_no, x, y):\n","  x_path = \"{}/testing/data_x_{}.npy\".format(path, part_no)\n","  y_path = '{}/testing/data_y_{}.npy'.format(path, part_no)\n","\n","  np.save(x_path, x)\n","  np.save(y_path, y)\n","\n","  print(\"Part\", part_no, \"saved\")\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqyYS1HWJ5bY"},"outputs":[],"source":["def predict(x, models):\n","  \"\"\"\n","  x : array of features\n","  models : keras CNN models for all classes\n","  \"\"\"\n","  y = []\n","\n","  for label, model in models.items():\n","    if model == None:\n","      y.append(np.zeros(x.shape[0]))\n","    else:\n","      y_pred = model.predict(x)\n","      y_pred = y_pred.reshape(y_pred.shape[0])\n","      y_pred = (y_pred > 0.5).astype('int8')\n","      y.append(y_pred)\n","\n","  y = np.array(y, dtype='int8')\n","\n","  return y.T\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5496,"status":"ok","timestamp":1605110117762,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"uHQM0JC2MSWu","outputId":"d3587d52-3a50-4ebf-b04f-750dde620f99"},"outputs":[],"source":["models = {}\n","for label in classes:\n","  try:\n","    models[label] = load_model(MY_DIR, label)\n","    # models[label] = model\n","  except:\n","    models[label] = None\n","\n","models.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUZCtwXSm-xc"},"outputs":[],"source":["def accuracy(y, y_pred):\n","  # take union and intersection\n","  union = np.count_nonzero(y + y_pred, axis=1)\n","  inter = np.count_nonzero(y * y_pred, axis=1)\n","\n","  sum = 0\n","  for u, i in zip(union, inter):\n","    if u != 0:\n","      sum += i/u\n","    else:\n","      sum += 1\n","\n","  return sum/y.shape[0]\n","\n","\n","def precision(y, y_pred):\n","  # take intersection for numerator and use y_pred for denominatior\n","  inter = np.count_nonzero(y * y_pred, axis=1)\n","  pred = np.count_nonzero(y_pred, axis=1)\n","\n","  sum = 0\n","  for num, den in zip(inter, pred):\n","    if den != 0:\n","      sum += num/den\n","    else:\n","      sum += 1\n","\n","  return sum/y.shape[0]\n","\n","def recall(y, y_pred):\n","  # take intersection for numerator and use y for denominatior\n","  inter = np.count_nonzero(y * y_pred, axis=1)\n","  y = np.count_nonzero(y, axis=1)\n","\n","  sum = 0\n","  for num, den in zip(inter, y):\n","    if den != 0:\n","      sum += num/den\n","    else:\n","      sum += 1\n","\n","  return sum/y.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213062,"status":"ok","timestamp":1605110384320,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"OX0aiczaFhNY","outputId":"a6d23ff2-d4ca-49b3-e7df-7e0e1cc96802"},"outputs":[],"source":["import gc\n","parts = [1,2,3]\n","\n","y = None\n","y_pred = None\n","\n","for part_no in parts:\n","  print('Processing part', part_no, '...')\n","  x, y_part = load_testing_data(MY_DIR, part_no)\n","  print('Data Loaded...')\n","\n","  y_pred_part = predict(x, models)\n","\n","  if y is None:\n","    y = y_part\n","    y_pred = y_pred_part\n","  else:\n","    y = np.concatenate((y, y_part), axis=0)\n","    y_pred = np.concatenate((y_pred, y_pred_part), axis=0)\n","\n","  del x\n","  del y_part\n","\n","  gc.collect()\n","\n","print(y.shape, y_pred.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1225,"status":"ok","timestamp":1605110547947,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"ccmQnErGFXyS","outputId":"56599b2d-8b1e-4bcd-fbca-ac631119fb38"},"outputs":[],"source":["\n","prec = precision(y, y_pred)\n","rec = recall(y, y_pred)\n","\n","\n","print('Precision :{:.2f}'.format(prec))\n","print('Recall :{:.2f}'.format(rec))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138460,"status":"ok","timestamp":1604985809960,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"MX4dSqKmIdiK","outputId":"1cf0282a-8a2f-48ef-c833-35608f412f7a"},"outputs":[],"source":["print(y.shape, y_pred.shape)"]},{"cell_type":"markdown","metadata":{"id":"YOEnZokYC775"},"source":["##Rough  Work "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lHQg3mXJYJO"},"outputs":[],"source":["for i in range(10,30):\n","  print('union',union[i])\n","  print('inter',inter[i])\n","  print('')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1220,"status":"ok","timestamp":1604989404843,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"ei8TCW4ZqvmP","outputId":"b266ee63-873a-453e-9586-aa15aae22d69"},"outputs":[],"source":["acc = accuracy(y, y_pred)\n","prec = precision(y, y_pred)\n","rec = recall(y, y_pred)\n","\n","print('Accuracy : {:.2f}'.format(acc))\n","print('Precision :{:.2f}'.format(pre))\n","print('Recall :{:.2f}'.format(rec))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1281,"status":"ok","timestamp":1604993000829,"user":{"displayName":"Manish Kumar","photoUrl":"","userId":"08779636860028061865"},"user_tz":-330},"id":"69XlV9MFOSNB","outputId":"2f5a91f5-56ee-45f2-87c8-2299af46244c"},"outputs":[],"source":["acc = accuracy(y, y_pred)\n","prec = precision(y, y_pred)\n","rec = recall(y, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHk-VzLnnt28"},"outputs":[],"source":["print('Accuracy : {:.2f}'.format(acc))\n","print('Precision :{:.2f}'.format(prec))\n","print('Recall :{:.2f}'.format(rec))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeL8NU6mPC1x"},"outputs":[],"source":["%reset -f"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3449,"status":"ok","timestamp":1605105366226,"user":{"displayName":"Harsh Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_erd1LY0mjqC3q-1cDkPe4pyzZbBtvyajN0E7=s64","userId":"05325981701283986423"},"user_tz":-330},"id":"MwwHRu0T-AMJ","outputId":"d4dede19-d657-451b-921a-2b0be227f475"},"outputs":[],"source":["import gc\n","pre=8.4\n","gc.collect()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"main.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
